# SIGN_LANGUAGE_RECOGNITION
Sign Language Recognition is an Python-based that translates hand gestures into text with speech using computer vision, and machine learning algorithms. It enhances communication for the deaf by recognizing gestures via cameras  and mapping them to meaningful words.
# Final Result
![image](https://github.com/user-attachments/assets/4bf94395-6268-439f-934b-121c23f8a938)
This is the output we got after running the code
![image](https://github.com/user-attachments/assets/cab01b86-7985-4438-9acb-bc92a6c72f53)
                Recognition of Hand gesture: In the above image it classifies the hand gesture as Letter – “L” 
![image](https://github.com/user-attachments/assets/b014246d-eb69-43ce-a85c-e76d7ddc1fa1)
                                                                      Recognition of Hand gesture: In the above image it classifies the hand gesture as Letter – “W”
# Table of Contents
[Overview](#overview)  
[Dataset](#dataset)  
[Installation](#installation)  
[Project Structure](#project-structure)  
[Data Preprocessing](#data-preprocessing)  
[Model Training](#model-training)  
[Evaluation](#evaluation)  
[Results](#results)  
[Contributing](#contributing)  
[License](#license) 

# Overview 
Sign Language Recognition is an python-based that translates hand gestures into text with speech, enabling communication for the deaf and hard-of-hearing. This project uses Convolutional Neural Networks (CNNs) to recognize and classify sign language gestures based on image or video input, improving accessibility and interaction.

# Dataset
 The dataset used in this project includes Hand gestures such as:
- Hand shape and finger positions  
- Movement and orientation of the hands  
- Background conditions and lighting variations  
- Gesture sequences for dynamic signs  
- Other relevant features  

The dataset contains features for accurately recognizing and classifying sign language gestures.

# Project-Structure


